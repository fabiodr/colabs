{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fabiodr/colabs/blob/main/Tutorial_Examples.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhUPEwGCCH4L"
      },
      "outputs": [],
      "source": [
        "!pip install voyageai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import voyageai\n",
        "os.environ['VOYAGE_API_KEY'] = \"<your secret key>\"\n",
        "vo = voyageai.Client(api_key=os.environ.get(\"VOYAGE_API_KEY\"),)"
      ],
      "metadata": {
        "id": "TSZuE9-xCLzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vectorize/embed the documents"
      ],
      "metadata": {
        "id": "sg4mlBegEstP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data\n",
        "documents = [\n",
        "    \"The Mediterranean diet emphasizes fish, olive oil, and vegetables, believed to reduce chronic diseases.\",\n",
        "    \"Photosynthesis in plants converts light energy into glucose and produces essential oxygen.\",\n",
        "    \"20th-century innovations, from radios to smartphones, centered on electronic advancements.\",\n",
        "    \"Rivers provide water, irrigation, and habitat for aquatic species, vital for ecosystems.\",\n",
        "\t  \"Appleâ€™s conference call to discuss fourth fiscal quarter results and business updates is scheduled for Thursday, November 2, 2023 at 2:00 p.m. PT / 5:00 p.m. ET.\",\n",
        "    \"Shakespeare's works, like 'Hamlet' and 'A Midsummer Night's Dream,' endure in literature.\"\n",
        "]"
      ],
      "metadata": {
        "id": "v2WGREqMCT-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embed the documents\n",
        "documents_embeddings = vo.embed(documents, model=\"voyage-3\", input_type=\"document\").embeddings"
      ],
      "metadata": {
        "id": "3MNM6zssCY_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are working with more than 128 documents, you will need to use a for loop to encode them:"
      ],
      "metadata": {
        "id": "2spUUzojq0Wb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A minimalist retrieval system\n",
        "\n",
        "The main feature of the embeddings is that the cosine similarity between two embeddings captures the semantic relatedness of the corresponding original passages. This allows us to use the embeddings to do semantic retrieval / search.\n",
        "\n",
        "Suppose the user sends a \"query\" (e.g., a question or a comment) to the chatbot:"
      ],
      "metadata": {
        "id": "D7LJAdmCC8rE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"When is Apple's conference call scheduled?\""
      ],
      "metadata": {
        "id": "V1NdERNLCiIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To find out the document that is most similar to the query among the existing data, we can first embed/vectorize the query:"
      ],
      "metadata": {
        "id": "Ah0NJ7icDMSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the embedding of the query\n",
        "query_embedding = vo.embed([query], model=\"voyage-3\", input_type=\"query\").embeddings[0]"
      ],
      "metadata": {
        "id": "fgxUNXRxDKKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Nearest neighbor Search:** We can find a few closest embeddings in the documents embeddings based on the cosine similarity, and retrieve the corresponding document using the nearest_neighbors function."
      ],
      "metadata": {
        "id": "hxa7bvMyDQWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "def k_nearest_neighbors(query_embedding, documents_embeddings, k=5):\n",
        "  query_embedding = np.array(query_embedding) # convert to numpy array\n",
        "  documents_embeddings = np.array(documents_embeddings) # convert to numpy array\n",
        "\n",
        "  # Reshape the query vector embedding to a matrix of shape (1, n) to make it compatible with cosine_similarity\n",
        "  query_embedding = query_embedding.reshape(1, -1)\n",
        "\n",
        "  # Calculate the similarity for each item in data\n",
        "  cosine_sim = cosine_similarity(query_embedding, documents_embeddings)\n",
        "\n",
        "  # Sort the data by similarity in descending order and take the top k items\n",
        "  sorted_indices = np.argsort(cosine_sim[0])[::-1]\n",
        "\n",
        "  # Take the top k related embeddings\n",
        "  top_k_related_indices = sorted_indices[:k]\n",
        "  top_k_related_embeddings = documents_embeddings[sorted_indices[:k]]\n",
        "  top_k_related_embeddings = [list(row[:]) for row in top_k_related_embeddings] # convert to list\n",
        "\n",
        "  return top_k_related_embeddings, top_k_related_indices"
      ],
      "metadata": {
        "id": "A6zhMgxbD6n0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the nearest neighbor algorithm to find the document with the highest similarity\n",
        "retrieved_embd, retrieved_embd_index = k_nearest_neighbors(query_embedding, documents_embeddings, k=1)\n",
        "retrieved_doc = [documents[index] for index in retrieved_embd_index]\n",
        "\n",
        "print(retrieved_doc)"
      ],
      "metadata": {
        "id": "FHRshmDVDTDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**$k$-nearest neighbors Search ($k$-NN):** It is often useful to retrieve not only the closest document but also the $k$ most closest documents. The k_nearest_neighbors algorithm enables us to achieve this. It is important to note that `nearest_neighbors` is special case of `k_nearest_neighbors` when $k=1$."
      ],
      "metadata": {
        "id": "7OZL9ft8DXRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the k-nearest neighbor algorithm to identify the top-k documents with the highest similarity\n",
        "retrieved_embds, retrieved_embd_indices = k_nearest_neighbors(query_embedding, documents_embeddings, k=3)\n",
        "retrieved_docs = [documents[index] for index in retrieved_embd_indices]\n",
        "\n",
        "print(retrieved_docs)"
      ],
      "metadata": {
        "id": "ct6imBPuDYNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Refinement with rerankers\n",
        "We can further refine our embedding-based retrieval with rerankers.  Here, a reranker reranks the documents for semantic relevance against the query and produces a more relevant and smaller set of documents for inputting to the generative model."
      ],
      "metadata": {
        "id": "eBG-2dhqWazk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reranking\n",
        "documents_reranked = vo.rerank(\n",
        "  query,\n",
        "  retrieved_docs,\n",
        "  model=\"rerank-lite-1\",\n",
        "  top_k=3\n",
        ")"
      ],
      "metadata": {
        "id": "1iXV5WnnWbjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that the reranker properly ranks the Apple conference call document as the most relevant to the query."
      ],
      "metadata": {
        "id": "Q4b6kd5LWjrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for r in documents_reranked.results:\n",
        "  print(f\"Document: {r.document}\")\n",
        "  print(f\"Relevance Score: {r.relevance_score}\")\n",
        "  print(f\"Index: {r.index}\")\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "id": "s5QdGbf8Wlo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take the document with the highest score\n",
        "retrieved_docs = documents_reranked.results[0].document\n",
        "print(retrieved_docs)"
      ],
      "metadata": {
        "id": "VPCtf2o8WnbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A minimalist RAG chatbot\n",
        "The [Retrieval-Augmented Generation](https://www.pinecone.io/learn/retrieval-augmented-generation/) (RAG) chatbot represents a cutting-edge approach in conversational artificial intelligence. RAG combines the powers of retrieval-based and generative methods to produce more accurate and contextually relevant responses. RAG can leverage a large corpora of text to retrieve relevant documents and then send those documents to language models, such as GPT-4, to generate replies. This methodology ensures that the chatbot's answers are both informed by vast amounts of information and tailored to the specifics of the user's query.\n",
        "\n",
        "Suppose you have implemented a semantic search system as described in the previous section, and as a result of the search process, you have retrieved the most relevant document, referred to as `retrieved_doc`. We can craft a prompt with this context which we can use as input to the language model."
      ],
      "metadata": {
        "id": "8ogz-IivEkaz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Take the retrieved document and use it as a prompt for the text generation model\n",
        "prompt = f\"Based on the information: '{retrieved_doc}', generate a response of {query}\""
      ],
      "metadata": {
        "id": "ZyxM-4LlBV7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you can utilize a text generation model like Claude 3.5 Sonnet to craft a response based on the provided query and the retrieved document."
      ],
      "metadata": {
        "id": "lP06bliqBZhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install anthropic\n",
        "!pip install anthropic"
      ],
      "metadata": {
        "id": "ym2vlz32BR7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import anthropic\n",
        "\n",
        "# Initialize Anthropic API\n",
        "client = anthropic.Anthropic(api_key=\"YOUR ANTHROPIC API KEY\")\n",
        "\n",
        "message = client.messages.create(\n",
        "    model=\"claude-3-5-sonnet-20240620\",\n",
        "    max_tokens=1024,\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(message.content[0].text)"
      ],
      "metadata": {
        "id": "0ljoBE2xBQrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " You can do the same with GPT-4o as well."
      ],
      "metadata": {
        "id": "2-xNNgdLC154"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install openai\n",
        "!pip install openai"
      ],
      "metadata": {
        "id": "P-8a2qcLnvAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "# Initialize OpenAI API\n",
        "client = OpenAI(api_key=\"YOUR OPENAI API KEY\")\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "CtjqvZcpDZn2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}