{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fabiodr/colabs/blob/main/finetune_LightOnOCR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDUsis12vK2l"
      },
      "source": [
        "# Fine-tune LightOnOCR on OCR Tasks\n",
        "\n",
        "In this notebook, we will fine-tune [LightOnOCR-1B](https://huggingface.co/lightonai/LightOnOCR-1B-1025) on a custom OCR dataset. LightOnOCR is a vision-language model specifically designed for OCR tasks.\n",
        "\n",
        "## Why Fine-tune LightOnOCR? üéØ\n",
        "\n",
        "LightOnOCR is an **end-to-end trainable model**, making it incredibly flexible for adaptation to specific use cases. Unlike traditional OCR pipelines that require complex multi-stage processing, LightOnOCR can be easily fine-tuned on:\n",
        "\n",
        "- **Specific domains** üìÑ ‚Äì Medical records, legal documents, receipts, forms, etc.\n",
        "- **Different languages** üåç ‚Äì Enhance performance on low-resource languages or specialized scripts\n",
        "- **Custom writing styles** ‚úçÔ∏è ‚Äì Historical documents, handwriting, or stylized fonts\n",
        "- **Domain-specific vocabulary** üè¢ ‚Äì Technical jargon, product names, or industry terminology\n",
        "\n",
        "This end-to-end approach means you can optimize the entire model for your specific task with just a few training examples, without needing to retrain separate detection, recognition, or layout analysis components.\n",
        "\n",
        "## Getting Started\n",
        "\n",
        "We use the IAM handwritten text dataset from [HuggingFaceM4/FineVision](https://huggingface.co/datasets/HuggingFaceM4/FineVision) as an example, but you can easily adapt this notebook to your own dataset.\n",
        "\n",
        "**Note:** This notebook supports multiple FineVision subsets (olmOCR-mix-0225-books, olmOCR-mix-0225-documents, and iam) üìö‚úçÔ∏è\n",
        "\n",
        "For more details about the model, see the [LightOnOCR blog post](https://huggingface.co/blog/lightonai/lightonocr).\n",
        "\n",
        "## Installation\n",
        "\n",
        "First, let's install the necessary libraries including the transformers fork with LightOnOCR support."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbPbUqBLvK2m"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U datasets accelerate\n",
        "!pip install -q -U \"pillow>=12.0.0\"\n",
        "!pip install -q -U git+https://github.com/baptiste-aubertin/transformers.git@main\n",
        "!pip install -q huggingface-hub==1.0.0\n",
        "!pip install -U bitsandbytes>=0.46.1\n",
        "!pip install -q jiwer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ja6hsIKWvK2n"
      },
      "source": [
        "## Load Dataset\n",
        "\n",
        "For this example, we'll use the **IAM handwriting dataset** from [HuggingFaceM4/FineVision](https://huggingface.co/datasets/HuggingFaceM4/FineVision) to finetune the model on handwritten text recognition ‚úçÔ∏è.\n",
        "\n",
        "**FineVision Dataset Structure:**\n",
        "- `images`: List of PIL Images\n",
        "- `texts`: List of conversation dicts with:\n",
        "  - `user`: Question/prompt(we ignore this one)\n",
        "  - `assistant`: OCR ground truth text\n",
        "- `source`: Dataset source identifier\n",
        "\n",
        "**Other available subsets:**\n",
        "- `olmOCR-mix-0225-books` - Books subset of olmOCR-mix-0225 üìö\n",
        "- `olmOCR-mix-0225-documents` - Document subset olmOCR-mix-0225 üìÑ\n",
        "- And many more! See the [FineVision dataset page](https://huggingface.co/datasets/HuggingFaceM4/FineVision) for all available datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_AgOn7YvK2n"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "\n",
        "# choose dataset subset\n",
        "finevision_subset = \"iam\"\n",
        "# finevision_subset = \"olmOCR-mix-0225-books\"\n",
        "# finevision_subset = \"olmOCR-mix-0225-documents\"\n",
        "\n",
        "train_ds = load_dataset('HuggingFaceM4/FineVision', finevision_subset, split='train[:85%]')\n",
        "val_ds = load_dataset('HuggingFaceM4/FineVision', finevision_subset, split='train[85%:95%]')\n",
        "test_ds = load_dataset('HuggingFaceM4/FineVision', finevision_subset, split='train[95%:]')\n",
        "\n",
        "print(f\"Training samples: {len(train_ds)}, Validation samples: {len(val_ds)}, Test samples: {len(test_ds)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cm6TwFIYvK2n"
      },
      "source": [
        "## Load Model and Processor\n",
        "\n",
        "We'll load the LightOnOCR model with full fine-tuning (optionally freezing parts of the model to reduce memory requirements)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxWTgisNvK2n"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoProcessor\n",
        "import torch\n",
        "\n",
        "model_id = \"lightonai/LightOnOCR-1B-1025\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "processor.tokenizer.padding_side = 'left'\n",
        "\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Note: this cell fails sometimes! but it's enough to restart the notebook and it works!\n",
        "\n"
      ],
      "metadata": {
        "id": "3eE9FCwVdAqQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TUjqhFnvK2n"
      },
      "outputs": [],
      "source": [
        "from transformers import LightOnOCRForConditionalGeneration\n",
        "\n",
        "model = LightOnOCRForConditionalGeneration.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    attn_implementation=\"sdpa\",\n",
        "    device_map=\"auto\",\n",
        ").to(device)\n",
        "\n",
        "# # freeze vision encoder, projector or language model to reduce memory requirements on Colab\n",
        "# for param in model.model.vision_encoder.parameters():\n",
        "#     param.requires_grad = False\n",
        "# print(f\"Vision encoder frozen: {param.requires_grad}\")\n",
        "# for param in model.model.vision_projection.parameters():\n",
        "#     param.requires_grad = False\n",
        "# print(f\"Vision projection frozen: {param.requires_grad}\")\n",
        "for param in model.model.language_model.parameters():\n",
        "    param.requires_grad = False\n",
        "print(f\"Language model frozen: {param.requires_grad}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8Q2YxjnvK2n"
      },
      "source": [
        "### Option 2: LoRA Fine-tuning\n",
        "\n",
        "Uncomment this cell to use LoRA instead of full fine-tuning. This is more memory efficient and recommended for limited GPU resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OowXNeSTvK2n"
      },
      "outputs": [],
      "source": [
        "# from transformers import LightOnOCRForConditionalGeneration, BitsAndBytesConfig\n",
        "# from peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training\n",
        "# import torch\n",
        "\n",
        "# bnb_config = BitsAndBytesConfig(\n",
        "#     load_in_4bit=True,\n",
        "#     bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "#     bnb_4bit_use_double_quant=True,\n",
        "#     bnb_4bit_quant_type=\"nf4\"\n",
        "# )\n",
        "\n",
        "# model = LightOnOCRForConditionalGeneration.from_pretrained(\n",
        "#     model_id,\n",
        "#     device_map=\"auto\",\n",
        "#     torch_dtype=torch.bfloat16,\n",
        "#     quantization_config=bnb_config\n",
        "# )\n",
        "\n",
        "# model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# # # convert vision encoder to bfloat16 to match input dtype\n",
        "# # if hasattr(model, 'model') and hasattr(model.model, 'vision_encoder'):\n",
        "# #     model.model.vision_encoder = model.model.vision_encoder.to(torch.bfloat16)\n",
        "\n",
        "# lora_config = LoraConfig(\n",
        "#     r=8,\n",
        "#     lora_alpha=16,\n",
        "#     target_modules=[\"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "#     lora_dropout=0.05,\n",
        "#     bias=\"none\",\n",
        "#     task_type=\"CAUSAL_LM\"\n",
        "# )\n",
        "\n",
        "# model = get_peft_model(model, lora_config)\n",
        "# model.print_trainable_parameters()\n",
        "\n",
        "# print(\"Model loaded with 4-bit quantization + LoRA\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ms-m81mjvK2n"
      },
      "source": [
        "## Prepare Data Collator\n",
        "\n",
        "The data collator prepares batches for training. It:\n",
        "1. Formats prompts with image tokens using chat template\n",
        "2. Processes images and text through the processor\n",
        "3. Creates labels for training (masking prompt tokens, only training on assistant response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjU5enKovK2n"
      },
      "outputs": [],
      "source": [
        "# assistant start pattern: <|im_end|>\\n<|im_start|>assistant\\n\n",
        "ASSISTANT_START_PATTERN = [151645, 1699, 151644, 77091, 1699]\n",
        "MAX_LENGTH = 1024\n",
        "LONGEST_EDGE = 700\n",
        "\n",
        "def collate_fn(examples):\n",
        "    batch_messages = []\n",
        "    batch_images = []\n",
        "\n",
        "    for example in examples:\n",
        "        example_images = example[\"images\"]\n",
        "        example_texts = example[\"texts\"]\n",
        "\n",
        "        assert len(example_images) == 1, f\"Expected 1 image per sample, got {len(example_images)}\"\n",
        "        assert len(example_texts) == 1, f\"Expected 1 text per sample, got {len(example_texts)}\"\n",
        "\n",
        "        image = example_images[0].convert(\"RGB\")\n",
        "        batch_images.append(image)\n",
        "\n",
        "        conversation = example_texts[0]\n",
        "        # strip extra whitespaces and newlines to avoid tokenization issues\n",
        "        assistant_text = conversation.get(\"assistant\", \"\").strip()\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"user\", \"content\": [{\"type\": \"image\"}]},\n",
        "            {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": assistant_text}]}\n",
        "        ]\n",
        "        batch_messages.append(messages)\n",
        "\n",
        "    if len(batch_images) == 0:\n",
        "        return None\n",
        "\n",
        "    texts = [\n",
        "        processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
        "        for messages in batch_messages\n",
        "    ]\n",
        "\n",
        "    inputs = processor(\n",
        "        text=texts,\n",
        "        images=batch_images,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "        size={\"longest_edge\": LONGEST_EDGE} # reduce due to memory requirements\n",
        "    )\n",
        "\n",
        "    labels = inputs[\"input_ids\"].clone()\n",
        "    pad_token_id = processor.tokenizer.pad_token_id\n",
        "\n",
        "    for i in range(len(labels)):\n",
        "        full_ids = inputs[\"input_ids\"][i].tolist()\n",
        "\n",
        "        # find where assistant content starts (after the assistant marker)\n",
        "        assistant_content_start = None\n",
        "        for idx in range(len(full_ids) - len(ASSISTANT_START_PATTERN)):\n",
        "            if full_ids[idx:idx+len(ASSISTANT_START_PATTERN)] == ASSISTANT_START_PATTERN:\n",
        "                assistant_content_start = idx + len(ASSISTANT_START_PATTERN)\n",
        "                break\n",
        "\n",
        "        if assistant_content_start is None:\n",
        "            print(f\"Warning: Could not find assistant marker in sample {i}\")\n",
        "            print(f\"Sample {i} failed. Text: {texts[i]}\")\n",
        "            labels[i, :] = -100\n",
        "        else:\n",
        "            # mask everything first\n",
        "            labels[i, :] = -100\n",
        "\n",
        "            # unmask from assistant content start to end\n",
        "            # this trains on: assistant text + EOS\n",
        "            for idx in range(assistant_content_start, len(full_ids)):\n",
        "                if full_ids[idx] == pad_token_id:\n",
        "                    break\n",
        "                labels[i, idx] = inputs[\"input_ids\"][i, idx]\n",
        "\n",
        "        # mask padding tokens\n",
        "        labels[i, inputs[\"input_ids\"][i] == pad_token_id] = -100\n",
        "\n",
        "    inputs[\"labels\"] = labels\n",
        "\n",
        "    # convert tensors to device with proper dtype\n",
        "    inputs['pixel_values'] = inputs['pixel_values'].to(torch.bfloat16)\n",
        "\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5cQW9EKvK2n"
      },
      "source": [
        "## Test the Collator\n",
        "\n",
        "Let's test the collator with a sample batch to ensure everything works correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qILcBiPFvK2o"
      },
      "outputs": [],
      "source": [
        "# test with a small batch\n",
        "test_batch = collate_fn([train_ds[0], train_ds[1]])\n",
        "print(\"Input shape:\", test_batch[\"input_ids\"].shape)\n",
        "print(\"Labels shape:\", test_batch[\"labels\"].shape)\n",
        "print(\"Pixel values shape:\", test_batch[\"pixel_values\"].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5Oel1QmvK2o"
      },
      "source": [
        "## Test Model Before Fine-tuning\n",
        "\n",
        "Let's run inference with the base model first to see how it performs on our dataset before fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUb-LIedvK2o"
      },
      "outputs": [],
      "source": [
        "def run_inference(image):\n",
        "    \"\"\"run inference on a single image\"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": [{\"type\": \"image\"}]}\n",
        "    ]\n",
        "\n",
        "    text = processor.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    inputs = processor(\n",
        "        text=[text],\n",
        "        images=[[image]],\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "        size={\"longest_edge\": LONGEST_EDGE},\n",
        "    ).to(device)\n",
        "    inputs['pixel_values'] = inputs['pixel_values'].to(torch.bfloat16)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=512,\n",
        "        do_sample=True,\n",
        "    )\n",
        "\n",
        "    input_length = inputs['input_ids'].shape[1]\n",
        "    generated_ids = outputs[0, input_length:]\n",
        "    generated_text = processor.tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "    return generated_text.strip()\n",
        "\n",
        "# test on a few samples\n",
        "print(\"Testing base model on validation samples:\\n\")\n",
        "for idx in range(3):\n",
        "    sample = test_ds[idx]\n",
        "    image = sample[\"images\"][0]\n",
        "    ground_truth = sample[\"texts\"][0].get(\"assistant\", \"\").strip()\n",
        "\n",
        "    prediction = run_inference(image)\n",
        "\n",
        "    print(f\"Sample {idx + 1}:\")\n",
        "    print(f\"Prediction  : {prediction}\")\n",
        "    print(f\"Ground truth: {ground_truth}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # display image\n",
        "    display(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoQk9O8Y4kfT"
      },
      "source": [
        "The model performs well overall but still makes some mistakes on handwritten crops ‚úçÔ∏è since it was primarily trained on full-page PDF documents üìÑ, not paragraph-level crops like these. However, we can finetune it to boost handwritten text recognition! üöÄ‚ú®"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTsrsyAfl51A"
      },
      "source": [
        "## Lets evaluate the model before training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuEOT_RUl79j"
      },
      "outputs": [],
      "source": [
        "from jiwer import cer, wer\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "def evaluate_model(model, dataset, num_samples=50, batch_size=8, description=\"Model\"):\n",
        "    model.eval()\n",
        "\n",
        "    predictions = []\n",
        "    ground_truths = []\n",
        "\n",
        "    print(f\"\\nEvaluating {description} on {num_samples} samples...\")\n",
        "\n",
        "    for start_idx in tqdm(range(0, min(num_samples, len(dataset)), batch_size)):\n",
        "        end_idx = min(start_idx + batch_size, num_samples, len(dataset))\n",
        "        batch_samples = [dataset[i] for i in range(start_idx, end_idx)]\n",
        "\n",
        "        batch_images = [[s[\"images\"][0]] for s in batch_samples]\n",
        "        batch_ground_truths = [s[\"texts\"][0][\"assistant\"].strip() for s in batch_samples]\n",
        "\n",
        "        messages = [{\"role\": \"user\", \"content\": [{\"type\": \"image\"}]}]\n",
        "        text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "        texts = [text] * len(batch_images)\n",
        "\n",
        "        inputs = processor(text=texts,\n",
        "                           images=batch_images,\n",
        "                           return_tensors=\"pt\",\n",
        "                           padding=True,\n",
        "                           truncation=True,\n",
        "                           max_length=MAX_LENGTH,\n",
        "                           size={\"longest_edge\": LONGEST_EDGE},\n",
        "                           ).to(device)\n",
        "        inputs['pixel_values'] = inputs['pixel_values'].to(torch.bfloat16)\n",
        "\n",
        "        outputs = model.generate(**inputs, max_new_tokens=512, do_sample=True)\n",
        "\n",
        "        input_length = inputs['input_ids'].shape[1]\n",
        "        generated_ids = outputs[:, input_length:]\n",
        "        batch_predictions = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "        batch_predictions = [p.strip() for p in batch_predictions]\n",
        "\n",
        "        predictions.extend(batch_predictions)\n",
        "        ground_truths.extend(batch_ground_truths)\n",
        "\n",
        "    cer_score = cer(ground_truths, predictions) * 100\n",
        "    wer_score = wer(ground_truths, predictions) * 100\n",
        "    perfect_matches = sum(1 for pred, gt in zip(predictions, ground_truths) if pred == gt)\n",
        "\n",
        "    print(f\"CER: {cer_score:.2f}% | WER: {wer_score:.2f}% | Perfect: {perfect_matches}/{num_samples}\")\n",
        "\n",
        "    for i in range(min(3, len(predictions))):\n",
        "        match = \"‚úÖ\" if predictions[i] == ground_truths[i] else \"‚ùå\"\n",
        "        print(f\"{match} Sample {i+1}: '{predictions[i]}' vs '{ground_truths[i]}'\")\n",
        "\n",
        "    return {\"cer\": cer_score, \"wer\": wer_score, \"perfect_matches\": perfect_matches}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-7F9okWUR0t"
      },
      "outputs": [],
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"BEFORE TRAINING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if isinstance(model, PeftModel):\n",
        "    with model.disable_adapter():\n",
        "        base_results = evaluate_model(model, test_ds, num_samples=100, batch_size=4, description=\"Base\")\n",
        "else:\n",
        "    base_results = evaluate_model(model, test_ds, num_samples=100, batch_size=4, description=\"Base\")\n",
        "\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3J8rjFhrvK2o"
      },
      "source": [
        "## Configure Training Arguments\n",
        "\n",
        "Set up the training configuration. Adjust based on your hardware and requirements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Es0YANV2vK2o"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "import os\n",
        "\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "torch.set_float32_matmul_precision('high')\n",
        "\n",
        "output_dir = f\"lightonocr-ft-{finevision_subset}\"\n",
        "use_bf16 = torch.cuda.is_available()\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=1,\n",
        "    # max_steps=100,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=6,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=6e-5,\n",
        "    weight_decay=0.0,\n",
        "    logging_steps=50,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=500,\n",
        "    save_total_limit=1,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    bf16=use_bf16,\n",
        "    fp16=False,\n",
        "    remove_unused_columns=False,\n",
        "    dataloader_pin_memory=False,\n",
        "    gradient_checkpointing=True,\n",
        "    optim=\"adamw_torch_fused\",\n",
        "    warmup_steps=10,\n",
        "    lr_scheduler_type=\"linear\",\n",
        ")\n",
        "\n",
        "print(f\"Output directory: {output_dir}\")\n",
        "print(f\"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1y4H4DzvK2o"
      },
      "source": [
        "## Initialize Trainer and Start Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VptQ_qW7vK2o"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "# use a smaller validation set on Colab\n",
        "val_ds_small = val_ds.select(range(100))\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds_small,\n",
        "    data_collator=collate_fn,\n",
        ")\n",
        "\n",
        "print(\"Starting training...\")\n",
        "print(f\"Number of training samples: {len(train_ds)}\")\n",
        "print(f\"Number of validation samples: {len(val_ds_small)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5Dl9yrPvK2o"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gRtPJRjXWem"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_steps = []\n",
        "train_losses = []\n",
        "eval_steps = []\n",
        "eval_losses = []\n",
        "\n",
        "for entry in trainer.state.log_history:\n",
        "    if 'loss' in entry:\n",
        "        train_steps.append(entry['step'])\n",
        "        train_losses.append(entry['loss'])\n",
        "    if 'eval_loss' in entry:\n",
        "        eval_steps.append(entry['step'])\n",
        "        eval_losses.append(entry['eval_loss'])\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_steps, train_losses, label='Training Loss', marker='o', linewidth=2)\n",
        "plt.plot(eval_steps, eval_losses, label='Validation Loss', marker='s', linewidth=2)\n",
        "plt.xlabel('Steps', fontsize=12)\n",
        "plt.ylabel('Loss', fontsize=12)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0E5XxhumN0-"
      },
      "outputs": [],
      "source": [
        "# evaluate the model after training\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"AFTER TRAINING\")\n",
        "finetuned_results = evaluate_model(model, test_ds, num_samples=100, batch_size=4, description=\"Finetuned\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6zDFnYpmSfW"
      },
      "outputs": [],
      "source": [
        "# comparison\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMPARISON\")\n",
        "print(f\"{'Metric':<20} {'Base':<12} {'Finetuned':<12} {'Change':<12}\")\n",
        "print(\"-\" * 56)\n",
        "print(f\"{'CER (%)':<20} {base_results['cer']:<12.2f} {finetuned_results['cer']:<12.2f} {base_results['cer']-finetuned_results['cer']:+.2f}\")\n",
        "print(f\"{'WER (%)':<20} {base_results['wer']:<12.2f} {finetuned_results['wer']:<12.2f} {base_results['wer']-finetuned_results['wer']:+.2f}\")\n",
        "print(f\"{'Perfect':<20} {base_results['perfect_matches']:<12} {finetuned_results['perfect_matches']:<12} {finetuned_results['perfect_matches']-base_results['perfect_matches']:+d}\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With 2 epochs training, we can get even better results:\n",
        "\n",
        "```bash\n",
        "=========================================================\n",
        "COMPARISON\n",
        "Metric               Base         Finetuned    Change\n",
        "--------------------------------------------------------\n",
        "CER (%)              37.15        1.95         +35.20\n",
        "WER (%)              41.89        5.07         +36.81\n",
        "Perfect              54           166          +112\n",
        "==========================================================\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "cwmPhBcp_i_F"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcCz7d0nvK2o"
      },
      "source": [
        "## Save and Push Model\n",
        "\n",
        "Save the fine-tuned model and optionally push to Hugging Face Hub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rF9o4sPRvK2o"
      },
      "outputs": [],
      "source": [
        "# save model and processor\n",
        "trainer.save_model(output_dir)\n",
        "processor.save_pretrained(output_dir)\n",
        "print(f\"Model saved to {output_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtkVU8szvK2m"
      },
      "source": [
        "## Authentication\n",
        "\n",
        "Authenticate to access the model and push your fine-tuned model to the Hub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-kNo1c_vK2m"
      },
      "outputs": [],
      "source": [
        "# from huggingface_hub import notebook_login, logout\n",
        "# logout()\n",
        "# notebook_login()\n",
        "# # optional: push to Hub\n",
        "# hub_model_id = \"staghado/LightOnOCR-1B-1025-ft-iam\"\n",
        "# trainer.push_to_hub(hub_model_id)\n",
        "# processor.push_to_hub(hub_model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dY3SKv3fvK2o"
      },
      "source": [
        "## Inference\n",
        "\n",
        "Test the fine-tuned model on new images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iErAreDvvK2o"
      },
      "outputs": [],
      "source": [
        "# test on a validation sample\n",
        "test_idx = 0\n",
        "test_sample = val_ds[test_idx]\n",
        "test_image = test_sample[\"images\"][0]\n",
        "\n",
        "print(\"Running inference...\")\n",
        "result = run_inference(test_image)\n",
        "\n",
        "print(\"\\n=== Generated Text ===\")\n",
        "print(result)\n",
        "\n",
        "print(\"\\n=== Ground Truth ===\")\n",
        "print(test_sample[\"texts\"][0].get(\"assistant\", \"\"))\n",
        "\n",
        "display(test_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OoLmLshuwUA4"
      },
      "outputs": [],
      "source": [
        "# Test on multiple validation samples\n",
        "print(\"=\"*50)\n",
        "print(\"Testing Finetuned Model\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "num_samples = 5\n",
        "for test_idx in range(num_samples):\n",
        "    test_sample = val_ds[test_idx]\n",
        "    test_image = test_sample[\"images\"][0]\n",
        "    ground_truth = test_sample[\"texts\"][0][\"assistant\"]\n",
        "\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Sample {test_idx + 1}:\")\n",
        "\n",
        "    # Run inference\n",
        "    result = run_inference(test_image)\n",
        "\n",
        "    print(f\"Prediction  : {result}\")\n",
        "    print(f\"Ground truth: {ground_truth}\")\n",
        "\n",
        "    # Calculate CER\n",
        "    from jiwer import cer\n",
        "    error_rate = cer([ground_truth], [result]) * 100\n",
        "    print(f\"CER: {error_rate:.2f}%\")\n",
        "\n",
        "    # Show if improved (you'll need base model predictions to compare)\n",
        "    if result.strip() == ground_truth.strip():\n",
        "        print(\"‚úÖ Perfect match!\")\n",
        "\n",
        "    # Optional: display image\n",
        "    # display(test_image)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Testing complete! üöÄ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogWfYn8MvK2o"
      },
      "source": [
        "## Convert Model for vLLM Compatibility\n",
        "\n",
        "If you want to use the model with vLLM afterwards, you need to update the config to use the the following model types beacuse the current vLLM implementation of LightOnOCR relies on them, this will be fixed soon so both Transformers and vLLM use the same names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnCepyZfvK2o"
      },
      "outputs": [],
      "source": [
        "# import json\n",
        "\n",
        "# config_path = f\"{output_dir}/config.json\"\n",
        "# with open(config_path, 'r') as f:\n",
        "#     config = json.load(f)\n",
        "\n",
        "# # update model types for vLLM compatibility\n",
        "# config['model_type'] = 'mistral3'\n",
        "# config['text_config']['model_type'] = 'qwen3'\n",
        "# config['vision_config']['model_type'] = 'pixtral'\n",
        "\n",
        "# with open(config_path, 'w') as f:\n",
        "#     json.dump(config, f, indent=2)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}